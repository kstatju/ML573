{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> ComS 573     </center></h1>\n",
    "<h1><center> Lab 2 </center></h1>\n",
    "<h1><center> Kanak Choudhury </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, I have used the following parameter combinations\n",
    "    \n",
    "    hidden_layers = [1,2,3]\n",
    "    hidden_units = [50, 64, 80]\n",
    "    num_epochs = [10, 50, 100]\n",
    "    btch_size = [128, 200, 300]\n",
    "    learning_rate = [0.1, 0.5, 0.9]\n",
    "    momentum = [.3, .5, 0.9]\n",
    "    loss_func = ['categorical_crossentropy']\n",
    "    data_scaling = ['Standardize', 'Normalize']\n",
    "    activation_func = ['relu', 'tanh']\n",
    "\n",
    "Also used 80% - 20% training - validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n",
      "numpy 1.18.1\n",
      "pandas 1.0.1\n",
      "sklearn 0.22.1\n",
      "matplotlib 3.1.3\n",
      "keras 2.3.1\n",
      "re 2.2.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import matplotlib\n",
    "import keras\n",
    "import re\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "\n",
    "print('python ' +sys.version)\n",
    "print('numpy '+ np.__version__)\n",
    "print('pandas '+ pd.__version__)\n",
    "print('sklearn '+ sklearn.__version__)\n",
    "print('matplotlib '+ matplotlib.__version__)\n",
    "print('keras '+ keras.__version__)\n",
    "print('re '+ re.__version__)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = 'D:/ISU/COMS 573 - Machine Learning/HW/Lab2/'\n",
    "train_model = False\n",
    "\n",
    "df_tr = pd.read_csv(path+'optdigits.tra',header=None)\n",
    "X_tr, y_tr = df_tr.loc[:,0:63], df_tr.loc[:,64]\n",
    "ccat = y_tr.unique().size\n",
    "\n",
    "df_ts = pd.read_csv(path+'optdigits.tes',header=None)\n",
    "X_ts,  y_ts  = df_ts.loc[:,0:63],  df_ts.loc[:,64]\n",
    "\n",
    "scaler = StandardScaler().fit(X_tr)\n",
    "normalizer = Normalizer().fit(X_tr)\n",
    "\n",
    "X_tr_std = scaler.transform(X_tr)\n",
    "X_tr_norm = normalizer.transform(X_tr)\n",
    "\n",
    "\n",
    "split = 0.8\n",
    "size = np.shape(X_tr)\n",
    "nsplit = int(np.floor(split*size[0]))\n",
    "\n",
    "y_train1 = np_utils.to_categorical(y_tr, ccat)\n",
    "y_train = y_train1[0:nsplit,:];\n",
    "y_val = y_train1[nsplit:size[0],:];\n",
    "y_test = np_utils.to_categorical(y_ts, ccat)\n",
    "\n",
    "\n",
    "X_train_std = X_tr_std[0:nsplit,:];\n",
    "X_val_std = X_tr_std[nsplit:size[0],:];\n",
    "X_test_std = scaler.transform(X_ts)\n",
    "\n",
    "\n",
    "X_train_norm = X_tr_norm[0:nsplit,:];\n",
    "X_val_norm = X_tr_norm[nsplit:size[0],:];\n",
    "X_test_norm = normalizer.transform(X_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skiped model fit\n"
     ]
    }
   ],
   "source": [
    "if train_model:\n",
    "    hidden_layers = [1,2,3]\n",
    "    hidden_units = [50, 64, 80]\n",
    "    num_epochs = [10, 50, 100]\n",
    "    btch_size = [128, 200, 300]\n",
    "    learning_rate = [0.1, 0.5, 0.9]\n",
    "    momentum = [.3, .5, 0.9]\n",
    "    loss_func = ['categorical_crossentropy']\n",
    "    data_scaling = ['Standardize', 'Normalize']\n",
    "    activation_func = ['relu', 'tanh']\n",
    "\n",
    "\n",
    "    def expand_grid(dictionary):\n",
    "       return pd.DataFrame([row for row in product(*dictionary.values())], \n",
    "                           columns=dictionary.keys())\n",
    "\n",
    "    dictionary = {'hidden_layers': hidden_layers,\n",
    "                  'hidden_units': hidden_units,\n",
    "                  'num_epochs': num_epochs, \n",
    "                  'batch_size': btch_size,\n",
    "                  'learning_rate': learning_rate,\n",
    "                  'momentum': momentum,\n",
    "                  'loss_func': loss_func,\n",
    "                  'data_scaling': data_scaling,\n",
    "                  'activation_func': activation_func}\n",
    "\n",
    "    prem1 = expand_grid(dictionary)\n",
    "    prem1 = prem1[~((prem1['activation_func'] == 'tanh') & (prem1['loss_func'] == 'mean_squared_error'))]\n",
    "    prem1['time'] = np.NaN\n",
    "    prem1['train_loss'] = np.NaN\n",
    "    prem1['validation_loss'] = np.NaN\n",
    "    prem1['test_loss'] = np.NaN\n",
    "    prem1['train_acc'] = np.NaN\n",
    "    prem1['validation_acc'] = np.NaN\n",
    "    prem1['test_acc'] = np.NaN\n",
    "    size_prem1 = prem1.shape\n",
    "    print(prem1.head())\n",
    "\n",
    "    ll = 0 \n",
    "    for j in range(0,2):\n",
    "        if j == 0:\n",
    "            X_train = X_train_std\n",
    "            X_val = X_val_std\n",
    "            X_test = X_test_std\n",
    "            listind = prem1[prem1['data_scaling'] == 'Standardize'].index.tolist()\n",
    "        else:\n",
    "            X_train = X_train_norm\n",
    "            X_val = X_val_norm\n",
    "            X_test = X_test_norm\n",
    "            listind = prem1[prem1['data_scaling'] == 'Normalize'].index.tolist()\n",
    "\n",
    "        for i in listind:\n",
    "            start = time. time()\n",
    "            if prem1.iloc[i,0] == 1:\n",
    "                model = Sequential()\n",
    "                model.add(Dense(prem1.iloc[i,1], input_dim=64, activation=prem1.iloc[i,8]))\n",
    "                model.add(Dense(ccat, activation='softmax'))\n",
    "\n",
    "            elif prem1.iloc[i,0] == 2:\n",
    "                model = Sequential()\n",
    "                model.add(Dense(prem1.iloc[i,1], input_dim=64, activation=prem1.iloc[i,8]))\n",
    "                model.add(Dense(prem1.iloc[i,1], activation=prem1.iloc[i,8]))\n",
    "                model.add(Dense(ccat, activation='softmax'))\n",
    "\n",
    "            elif prem1.iloc[i,0] == 3:\n",
    "                model = Sequential()\n",
    "                model.add(Dense(prem1.iloc[i,1], input_dim=64, activation=prem1.iloc[i,8]))\n",
    "                model.add(Dense(prem1.iloc[i,1], activation=prem1.iloc[i,8]))\n",
    "                model.add(Dense(prem1.iloc[i,1], activation=prem1.iloc[i,8]))\n",
    "                model.add(Dense(ccat, activation='softmax'))\n",
    "\n",
    "            else:\n",
    "                model = Sequential()\n",
    "                model.add(Dense(prem1.iloc[i,1], input_dim=64, activation=prem1.iloc[i,8]))\n",
    "                model.add(Dense(prem1.iloc[i,1], activation=prem1.iloc[i,8]))\n",
    "                model.add(Dense(prem1.iloc[i,1], activation=prem1.iloc[i,8]))\n",
    "                model.add(Dense(prem1.iloc[i,1], activation=prem1.iloc[i,8]))\n",
    "                model.add(Dense(ccat, activation='softmax'))\n",
    "\n",
    "\n",
    "            es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=0, patience=200)\n",
    "            mc = ModelCheckpoint('best_model', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
    "\n",
    "\n",
    "            optimizer1 = optimizers.SGD(lr=prem1.iloc[i,4], momentum=prem1.iloc[i,5])\n",
    "            model.compile(optimizer=optimizer1, loss=prem1.iloc[i,6], metrics=['accuracy'])\n",
    "            fit1 = model.fit(X_train,y_train, batch_size=prem1.iloc[i,3], epochs=prem1.iloc[i,2], \n",
    "                             validation_data=(X_val,y_val), callbacks=[es, mc], verbose = 0)\n",
    "            fit = load_model('best_model')\n",
    "            end = time.time()\n",
    "            train_accuracy = fit.evaluate(X_train, y_train, verbose=0)\n",
    "            val_accuracy = fit.evaluate(X_val, y_val, verbose=0)\n",
    "            test_accuracy = fit.evaluate(X_test, y_test, verbose=0)\n",
    "            prem1.iloc[i, 9:16] = [end-start, train_accuracy[0], val_accuracy[0], test_accuracy[0], \n",
    "                                  train_accuracy[1], val_accuracy[1], test_accuracy[1]]\n",
    "\n",
    "\n",
    "            del model, es, mc, optimizer1, fit, fit1\n",
    "            gc.collect()\n",
    "            ll = ll+1\n",
    "            sys.stdout.write(\"\\r Progress: %.2f%%\" %round(float(ll)/size_prem1[0]*100,2))\n",
    "            sys.stdout.flush()\n",
    "else:\n",
    "    print('skiped model fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best 10 hyper-parameter combination for tanh:\n",
      "       hidden_layers  hiden_units  num_epochs  batch_size  learning_rate  \\\n",
      "1210              3           64          50         200            0.1   \n",
      "892               2           80          50         200            0.5   \n",
      "928               2           80         100         128            0.5   \n",
      "1220              3           64          50         200            0.9   \n",
      "800               2           64         100         300            0.5   \n",
      "1090              3           50         100         128            0.5   \n",
      "1270              3           64         100         200            0.5   \n",
      "623               2           50         100         200            0.5   \n",
      "946               2           80         100         200            0.5   \n",
      "1106              3           50         100         200            0.5   \n",
      "\n",
      "      momentum                 loss_func data_scaling activation_func  \\\n",
      "1210       0.9  categorical_crossentropy  Standardize            tanh   \n",
      "892        0.9  categorical_crossentropy  Standardize            tanh   \n",
      "928        0.9  categorical_crossentropy  Standardize            tanh   \n",
      "1220       0.5  categorical_crossentropy  Standardize            tanh   \n",
      "800        0.5  categorical_crossentropy  Standardize            tanh   \n",
      "1090       0.9  categorical_crossentropy  Standardize            tanh   \n",
      "1270       0.9  categorical_crossentropy  Standardize            tanh   \n",
      "623        0.9  categorical_crossentropy    Normalize            tanh   \n",
      "946        0.9  categorical_crossentropy  Standardize            tanh   \n",
      "1106       0.5  categorical_crossentropy  Standardize            tanh   \n",
      "\n",
      "         time  train_loss  validation_loss  test_loss  train_acc  \\\n",
      "1210   7.0783      0.0016           0.0789     0.1434     1.0000   \n",
      "892    5.1075      0.0041           0.0738     0.1368     0.9993   \n",
      "928    8.9442      0.0001           0.0987     0.1527     1.0000   \n",
      "1220   5.9771      0.0009           0.0469     0.1319     1.0000   \n",
      "800    6.6442      0.0088           0.0615     0.1275     1.0000   \n",
      "1090   9.1255      0.0001           0.0843     0.1729     1.0000   \n",
      "1270   8.4695      0.0000           0.1137     0.1811     1.0000   \n",
      "623   10.4227      0.0028           0.0558     0.1528     1.0000   \n",
      "946    7.5301      0.0014           0.0753     0.1495     1.0000   \n",
      "1106   7.9243      0.0087           0.0713     0.1158     0.9997   \n",
      "\n",
      "      validation_acc  test_acc  \n",
      "1210          0.9817    0.9699  \n",
      "892           0.9804    0.9694  \n",
      "928           0.9856    0.9688  \n",
      "1220          0.9869    0.9677  \n",
      "800           0.9856    0.9672  \n",
      "1090          0.9856    0.9672  \n",
      "1270          0.9804    0.9672  \n",
      "623           0.9908    0.9666  \n",
      "946           0.9830    0.9666  \n",
      "1106          0.9817    0.9666  \n",
      "\n",
      " Best 10 hyper-parameter combination for relu:\n",
      "       hidden_layers  hiden_units  num_epochs  batch_size  learning_rate  \\\n",
      "1430              2           64          50         128            0.9   \n",
      "486               1           64          50         200            0.5   \n",
      "782               1           80          50         128            0.9   \n",
      "1290              2           50         100         300            0.9   \n",
      "1522              2           64         100         128            0.1   \n",
      "2766              3           80          50         200            0.9   \n",
      "2854              3           80         100         200            0.1   \n",
      "2823              3           80         100         128            0.5   \n",
      "1427              2           64          50         128            0.5   \n",
      "1571              2           64         100         200            0.5   \n",
      "\n",
      "      momentum                 loss_func data_scaling activation_func  \\\n",
      "1430       0.3  categorical_crossentropy  Standardize            relu   \n",
      "486        0.5  categorical_crossentropy  Standardize            relu   \n",
      "782        0.3  categorical_crossentropy  Standardize            relu   \n",
      "1290       0.5  categorical_crossentropy  Standardize            relu   \n",
      "1522       0.9  categorical_crossentropy  Standardize            relu   \n",
      "2766       0.5  categorical_crossentropy  Standardize            relu   \n",
      "2854       0.9  categorical_crossentropy  Standardize            relu   \n",
      "2823       0.3  categorical_crossentropy    Normalize            relu   \n",
      "1427       0.9  categorical_crossentropy    Normalize            relu   \n",
      "1571       0.9  categorical_crossentropy    Normalize            relu   \n",
      "\n",
      "         time  train_loss  validation_loss  test_loss  train_acc  \\\n",
      "1430   5.8781      0.0013           0.0781     0.1324     1.0000   \n",
      "486    5.0245      0.0110           0.0740     0.1119     0.9997   \n",
      "782    4.8596      0.0016           0.0455     0.1397     1.0000   \n",
      "1290   6.5375      0.0030           0.0699     0.1386     1.0000   \n",
      "1522   8.1272      0.0019           0.0760     0.1469     1.0000   \n",
      "2766   8.6308      0.0009           0.0869     0.1967     1.0000   \n",
      "2854  17.2453      0.0011           0.0811     0.1428     1.0000   \n",
      "2823  61.7760      0.0106           0.0743     0.1560     0.9974   \n",
      "1427  64.7441      0.0025           0.0735     0.1666     0.9997   \n",
      "1571  32.2653      0.0020           0.0926     0.1864     0.9997   \n",
      "\n",
      "      validation_acc  test_acc  \n",
      "1430          0.9869    0.9699  \n",
      "486           0.9817    0.9694  \n",
      "782           0.9882    0.9672  \n",
      "1290          0.9856    0.9672  \n",
      "1522          0.9830    0.9672  \n",
      "2766          0.9817    0.9672  \n",
      "2854          0.9843    0.9672  \n",
      "2823          0.9856    0.9666  \n",
      "1427          0.9869    0.9661  \n",
      "1571          0.9869    0.9661  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train_model:\n",
    "    prem1.to_csv (path+'res_1b.csv', index = False, header=True)\n",
    "else:\n",
    "    prem221 = pd.read_csv(path+'res_1b.csv',header=0)\n",
    "    prem222 = pd.read_csv(path+'res_1a.csv',header=0)\n",
    "    \n",
    "    prem222 = prem222[((prem222['activation_func']=='relu') & (prem222['loss_func']=='categorical_crossentropy'))]\n",
    "    prem1 = prem221.append(prem222)\n",
    "    del prem221, prem222\n",
    "\n",
    "    \n",
    "top10_relu = prem1[prem1['activation_func'] == 'relu'].nlargest(10,'test_acc')\n",
    "top10_tanh = prem1[prem1['activation_func'] == 'tanh'].nlargest(10,'test_acc')\n",
    "print('\\n Best 10 hyper-parameter combination for tanh:\\n', round(top10_tanh, 4))\n",
    "print('\\n Best 10 hyper-parameter combination for relu:\\n', round(top10_relu, 4))\n",
    "\n",
    "plt.hist([prem1[prem1['activation_func'] == 'tanh'].iloc[:,9], \n",
    "          prem1[prem1['activation_func'] == 'relu'].iloc[:,9]], \n",
    "         bins=300, density=True, alpha=0.5, label=['tanh', 'relu'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Distribution of time to fit models')\n",
    "plt.xlim(0, 200)\n",
    "plt.show()\n",
    "\n",
    "plt.hist([prem1[prem1['activation_func'] == 'tanh'].iloc[:,15], \n",
    "          prem1[prem1['activation_func'] == 'relu'].iloc[:,15]], \n",
    "         density=True, alpha=0.5, label=['tanh', 'relu'])\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Distribution of test accuracy')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean and Variance of fitted time:\n",
      " tanh: Mean = 8.74, var = 129.97\n",
      " relu: Mean = 37.11, var = 70030.07\n",
      "\n",
      "Mean and Variance of test accuracy:\n",
      " tanh: Mean = 0.8627, var = 0.0587\n",
      " relu: Mean = 0.8520, var = 0.0515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aaa = prem1[prem1['activation_func'] == 'tanh'].iloc[:,9]\n",
    "bbb = prem1[prem1['activation_func'] == 'relu'].iloc[:,9]\n",
    "print(\"Mean and Variance of fitted time:\\n tanh: Mean = %.2f, \\\n",
    "var = %.2f\\n relu: Mean = %.2f, \\\n",
    "var = %.2f\\n\" %(np.mean(aaa), np.var(aaa), np.mean(bbb), np.var(bbb)))\n",
    "\n",
    "aaa = prem1[prem1['activation_func'] == 'tanh'].iloc[:,15]\n",
    "bbb = prem1[prem1['activation_func'] == 'relu'].iloc[:,15]\n",
    "print(\"Mean and Variance of test accuracy:\\n tanh: Mean = %.4f, \\\n",
    "var = %.4f\\n relu: Mean = %.4f, \\\n",
    "var = %.4f\\n\" %(np.mean(aaa), np.var(aaa), np.mean(bbb), np.var(bbb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Results For tanh Activation Function\n",
      "******************************************\n",
      "\n",
      "For hyper-parameters:\n",
      " hidden_layers                             3\n",
      "hiden_units                              64\n",
      "num_epochs                               50\n",
      "batch_size                              200\n",
      "learning_rate                           0.1\n",
      "momentum                                0.9\n",
      "loss_func          categorical_crossentropy\n",
      "data_scaling                    Standardize\n",
      "activation_func                        tanh\n",
      "time                                7.07828\n",
      "train_loss                       0.00162618\n",
      "validation_loss                   0.0788507\n",
      "test_loss                           0.14345\n",
      "train_acc                                 1\n",
      "validation_acc                     0.981699\n",
      "test_acc                            0.96995\n",
      "Name: 1210, dtype: object\n",
      "\n",
      " Time needed: 5.49\n",
      "\n",
      " Test Accuracy: 95.16%\n",
      "\n",
      " Train confusion matrix: \n",
      " [[374   0   0   0   1   0   1   0   0   0]\n",
      " [  0 384   0   0   0   0   1   0   2   2]\n",
      " [  1   0 378   1   0   0   0   0   0   0]\n",
      " [  0   1   0 384   0   3   0   0   0   1]\n",
      " [  0   0   0   0 384   0   1   1   0   1]\n",
      " [  0   0   1   0   1 370   0   0   0   4]\n",
      " [  0   2   0   0   1   0 373   0   1   0]\n",
      " [  0   0   0   1   0   1   0 385   0   0]\n",
      " [  0   2   0   0   0   0   1   0 376   1]\n",
      " [  0   1   0   2   6   1   0   0   2 370]]\n",
      "\n",
      " Class Accuracy for Training Data is:\n",
      "Class 0: 99.47%\n",
      "Class 1: 98.71%\n",
      "Class 2: 99.47%\n",
      "Class 3: 98.71%\n",
      "Class 4: 99.22%\n",
      "Class 5: 98.40%\n",
      "Class 6: 98.94%\n",
      "Class 7: 99.48%\n",
      "Class 8: 98.95%\n",
      "Class 9: 96.86%\n",
      "\n",
      " Test confusion matrix: \n",
      " [[177   0   0   0   1   0   0   0   0   0]\n",
      " [  0 175   0   0   0   0   4   0   1   2]\n",
      " [  0   5 164   6   0   0   1   0   1   0]\n",
      " [  0   0   5 176   0   1   0   0   0   1]\n",
      " [  0   1   0   0 178   0   0   0   2   0]\n",
      " [  0   0   0   1   0 179   1   0   0   1]\n",
      " [  0   1   0   0   2   0 177   0   1   0]\n",
      " [  0   0   0   0   2  11   0 163   1   2]\n",
      " [  0   7   0   1   0   3   1   0 154   8]\n",
      " [  0   1   1   2   3   3   0   0   3 167]]\n",
      "\n",
      " Class Accuracy for Testing Data is:\n",
      "Class 0: 99.44%\n",
      "Class 1: 96.15%\n",
      "Class 2: 92.66%\n",
      "Class 3: 96.17%\n",
      "Class 4: 98.34%\n",
      "Class 5: 98.35%\n",
      "Class 6: 97.79%\n",
      "Class 7: 91.06%\n",
      "Class 8: 88.51%\n",
      "Class 9: 92.78%\n",
      "**********************************\n",
      "\n",
      "\n",
      " Results For relu Activation Function\n",
      "******************************************\n",
      "\n",
      "For hyper-parameters:\n",
      " hidden_layers                             2\n",
      "hiden_units                              64\n",
      "num_epochs                               50\n",
      "batch_size                              128\n",
      "learning_rate                           0.9\n",
      "momentum                                0.3\n",
      "loss_func          categorical_crossentropy\n",
      "data_scaling                    Standardize\n",
      "activation_func                        relu\n",
      "time                                5.87814\n",
      "train_loss                       0.00131139\n",
      "validation_loss                   0.0780658\n",
      "test_loss                          0.132371\n",
      "train_acc                                 1\n",
      "validation_acc                     0.986928\n",
      "test_acc                            0.96995\n",
      "Name: 1430, dtype: object\n",
      "\n",
      " Time needed: 5.81\n",
      "\n",
      " Test Accuracy: 96.72%\n",
      "\n",
      " Train confusion matrix: \n",
      " [[376   0   0   0   0   0   0   0   0   0]\n",
      " [  0 389   0   0   0   0   0   0   0   0]\n",
      " [  0   0 378   1   0   0   1   0   0   0]\n",
      " [  0   1   0 387   0   1   0   0   0   0]\n",
      " [  0   0   0   0 386   0   1   0   0   0]\n",
      " [  0   0   0   0   0 376   0   0   0   0]\n",
      " [  0   0   0   0   0   0 377   0   0   0]\n",
      " [  0   0   0   1   0   1   0 385   0   0]\n",
      " [  0   0   0   1   0   0   0   0 379   0]\n",
      " [  0   0   0   1   0   0   0   0   1 380]]\n",
      "\n",
      " Class Accuracy for Training Data is:\n",
      "Class 0: 100.00%\n",
      "Class 1: 100.00%\n",
      "Class 2: 99.47%\n",
      "Class 3: 99.49%\n",
      "Class 4: 99.74%\n",
      "Class 5: 100.00%\n",
      "Class 6: 100.00%\n",
      "Class 7: 99.48%\n",
      "Class 8: 99.74%\n",
      "Class 9: 99.48%\n",
      "\n",
      " Test confusion matrix: \n",
      " [[178   0   0   0   0   0   0   0   0   0]\n",
      " [  0 179   0   0   0   0   3   0   0   0]\n",
      " [  0   1 171   4   0   0   0   0   1   0]\n",
      " [  1   0   7 172   0   1   0   0   0   2]\n",
      " [  0   2   0   0 176   0   0   0   3   0]\n",
      " [  0   1   0   0   0 180   0   0   0   1]\n",
      " [  0   1   0   0   3   0 176   0   1   0]\n",
      " [  0   0   0   0   1   2   0 173   1   2]\n",
      " [  0   4   0   2   1   0   0   0 160   7]\n",
      " [  0   0   1   1   0   4   0   0   1 173]]\n",
      "\n",
      " Class Accuracy for Testing Data is:\n",
      "Class 0: 100.00%\n",
      "Class 1: 98.35%\n",
      "Class 2: 96.61%\n",
      "Class 3: 93.99%\n",
      "Class 4: 97.24%\n",
      "Class 5: 98.90%\n",
      "Class 6: 97.24%\n",
      "Class 7: 96.65%\n",
      "Class 8: 91.95%\n",
      "Class 9: 96.11%\n",
      "**********************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    if i==0:\n",
    "        top10 = top10_tanh\n",
    "        print(\"\\n Results For tanh Activation Function\")\n",
    "        print(\"******************************************\\n\")\n",
    "    else:\n",
    "        top10 = top10_relu\n",
    "        print(\"\\n Results For relu Activation Function\")\n",
    "        print(\"******************************************\\n\")\n",
    "\n",
    "\n",
    "    if top10.iloc[0,7] == 'Standardize':\n",
    "        X_train = X_train_std\n",
    "        X_val = X_val_std\n",
    "        X_test = X_test_std\n",
    "    else:\n",
    "        X_train = X_train_norm\n",
    "        X_val = X_val_norm\n",
    "        X_test = X_test_norm\n",
    "\n",
    "\n",
    "    start = time. time()\n",
    "    if top10.iloc[0,0] == 1:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(top10.iloc[0,1], input_dim=64, activation=top10.iloc[0,8]))\n",
    "        model.add(Dense(ccat, activation='softmax'))\n",
    "\n",
    "    elif top10.iloc[0,0] == 2:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(top10.iloc[0,1], input_dim=64, activation=top10.iloc[0,8]))\n",
    "        model.add(Dense(top10.iloc[0,1], activation=top10.iloc[0,8]))\n",
    "        model.add(Dense(ccat, activation='softmax'))\n",
    "\n",
    "    elif top10.iloc[0,0] == 3:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(top10.iloc[0,1], input_dim=64, activation=top10.iloc[0,8]))\n",
    "        model.add(Dense(top10.iloc[0,1], activation=top10.iloc[0,8]))\n",
    "        model.add(Dense(top10.iloc[0,1], activation=top10.iloc[0,8]))\n",
    "        model.add(Dense(ccat, activation='softmax'))\n",
    "\n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(top10.iloc[0,1], input_dim=64, activation=top10.iloc[0,8]))\n",
    "        model.add(Dense(top10.iloc[0,1], activation=top10.iloc[0,8]))\n",
    "        model.add(Dense(top10.iloc[0,1], activation=top10.iloc[0,8]))\n",
    "        model.add(Dense(top10.iloc[0,1], activation=top10.iloc[0,8]))\n",
    "        model.add(Dense(ccat, activation='softmax'))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=0, patience=200)\n",
    "    mc = ModelCheckpoint('best_model', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
    "\n",
    "    optimizer1 = optimizers.SGD(lr=top10.iloc[0,4], momentum=top10.iloc[0,5])\n",
    "    model.compile(optimizer=optimizer1, loss=top10.iloc[0,6], metrics=['accuracy'])\n",
    "    fit1 = model.fit(X_train,y_train, batch_size=top10.iloc[0,3], epochs=top10.iloc[0,2], \n",
    "                     validation_data=(X_val,y_val), callbacks=[es, mc], verbose = 0)\n",
    "    fit = load_model('best_model')\n",
    "    end = time.time()\n",
    "\n",
    "    train_accuracy = fit.evaluate(X_train, y_train, verbose=0)\n",
    "    val_accuracy = fit.evaluate(X_val, y_val, verbose=0)\n",
    "    test_accuracy = fit.evaluate(X_test, y_test, verbose=0)\n",
    "    final_res = [end-start, train_accuracy[0], val_accuracy[0], test_accuracy[0], \n",
    "                 train_accuracy[1], val_accuracy[1], test_accuracy[1]]\n",
    "    \n",
    "    if top10.iloc[0,7] == 'Standardize':\n",
    "        X_train11 = X_tr_std\n",
    "        X_test = X_test_std\n",
    "        y_train11 = y_train1\n",
    "    else:\n",
    "        X_train11 = X_tr_norm\n",
    "        X_test = X_test_norm\n",
    "        y_train11 = y_train1\n",
    "\n",
    "    print(\"For hyper-parameters:\\n\",top10.iloc[0,:])\n",
    "    print(\"\\n Time needed: %.2f\" % (end-start))\n",
    "    scores = fit.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"\\n Test Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "    A = fit.predict(X_train11)\n",
    "    cm = confusion_matrix(y_train11.argmax(axis=1), A.argmax(axis=1))\n",
    "    print(\"\\n Train confusion matrix: \\n\", cm)\n",
    "    acc_train = np.diagonal(cm)/cm.sum(axis=1)\n",
    "    print(\"\\n Class Accuracy for Training Data is:\")\n",
    "    for i in range(10):\n",
    "        print('Class %d: %.2f%%' %(i, acc_train[i]*100))\n",
    "\n",
    "    A = fit.predict(X_test)\n",
    "    cm = confusion_matrix(y_test.argmax(axis=1), A.argmax(axis=1))\n",
    "    print(\"\\n Test confusion matrix: \\n\", cm)\n",
    "    acc_test = np.diagonal(cm)/cm.sum(axis=1)\n",
    "    print(\"\\n Class Accuracy for Testing Data is:\")\n",
    "    for i in range(10):\n",
    "        print('Class %d: %.2f%%' %(i, acc_test[i]*100))\n",
    "    print(\"**********************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the time distribution, $tanh$ activation function takes shorter time than $relu$ activation function that can be confirmed by the smaller mean and variance for $tanh$ than $relu$. \n",
    "\n",
    "However, based on test accuracy distributions, $tanh$ activation function is less sensitive on the hyper-parameters than the $relu$ activation function for this data because from the histogram we can see that for most hyper-parameters combination $tanh$ activation function produces test accuracy that is close to highest test accuracy. Additionally, the average test accuracy for all combinations of hyper-parameter is higher for $tanh$ compare to $relu$ and lower variance for $tanh$ than $relu$. This also supports that $tanh$ activation function is less sensitive on the hyper-parameters than the $relu$ for this data. Note that, for this experiment, I have considered same hyper-parameter combinations for $tanh$ and $relu$ activation functions.\n",
    "\n",
    "It is found that using $relu$ activation function with 2 hidden layers, 64 units, number of epochs 50, batch size 128, learning rate 0.9 and momentum 0.3 has the highest test accuracy (around 96.00%), although $tanh$ activation function gives almost same test accuracy. Note that, this model was fitted based on only 1-fold cross validation with no repeated sample. It might be different if we use repeated $k$ fold cross validation.\n",
    "\n",
    "Training accuracy for all classes are almost 100%. However, test accuracy for all classes are around 96%. Class 0 has the highest test accuracy and class 8 has the lowest accuracy for $relu$ activation function. Also, almost similar pattern has been found for the $tanh$ activation function. Overall classication accuracy, class accuracy, and confusion matrix for both training and testing data are given in above tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important References:\n",
    "\n",
    "1. https://towardsdatascience.com/building-our-first-neural-network-in-keras-bdc8abbc17f5\n",
    "\n",
    "2. https://towardsdatascience.com/building-a-deep-learning-model-using-keras-1548ca149d37\n",
    "\n",
    "3. https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "4. https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5\n",
    "\n",
    "5. https://towardsdatascience.com/convolutional-neural-networks-for-beginners-practical-guide-with-python-and-keras-dc688ea90dca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonCPU",
   "language": "python",
   "name": "pythoncpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
